{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "443c6dcb",
   "metadata": {},
   "source": [
    "# Case Study\n",
    "\n",
    "## Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cada4e",
   "metadata": {},
   "source": [
    "### Git & version control\n",
    "\n",
    "1. Create a Github repository called \"ames-housing-analysis\".\n",
    "1. Copy the ames.csv data from the `data/` directory into this repository.\n",
    "1. Update the README with a short synopsis of this repo.\n",
    "1. Create a folder called `notebooks/`\n",
    "1. Add, commit, and push what you have so far. Verify in that it appears in GitHub on your repository page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43125c4",
   "metadata": {},
   "source": [
    "### Exploratory data analysis\n",
    "\n",
    "1. In the repo's `notebooks/` folder, create a new notebook: `eda.ipynb`.\n",
    "2. Load the ames.csv data.\n",
    "3. Assess the distribution of the response variable (`Sale_Price`).\n",
    "4. How many features are numeric vs. categorical?\n",
    "5. Pick a numeric feature that you believe would be influential on a home's `Sale_Price`. Assess the distribution of the numeric feature. Assess the relationship between that feature and the `Sale_Price`.\n",
    "6. Pick a categorical feature that you believe would be influential on a home's `Sale_Price`. Assess the distribution of the categorical feature. Assess the relationship between that feature and the `Sale_Price`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90db6075",
   "metadata": {},
   "source": [
    "### Modular code & Scikit-learn model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626052ab",
   "metadata": {},
   "source": [
    "1. Copy `my_module.py` (that we created together) into the notebooks folder.\n",
    "2. Import your module and use `get_features_and_target` to load the numeric features of the Ames data, along with the \"Sale_Price\" as a target column.\n",
    "\n",
    "With your features and target prepared:\n",
    "1. Split the data into training and test sets. Use 75% of the data for training and 25% for testing.\n",
    "2. Fit a default `sklearn.neighbors.KNeighborsRegressor` model on the training data and score on the test data. Note that scoring on regression models provides the $R^2$.\n",
    "3. Fit a default `sklearn.linear_model.LinearRegression` model on the training data and score on the test data.\n",
    "4. Fit a default `sklearn.ensemble.RandomForestRegressor` model on the training data and score on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1737cd83",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1b0caf",
   "metadata": {},
   "source": [
    "1. Fill in the blanks to standardize the numeric features and then apply a linear regression model. Does standardizing the numeric features improve the linear regression's $R^2$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6be7a3",
   "metadata": {
    "tags": [
     "ci-skip"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import ________\n",
    "\n",
    "lm_model_scaled = make_pipeline(__________, LinearRegression())\n",
    "lm_model_scaled.fit(X_train, y_train)\n",
    "lm_model_scaled.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57871c5",
   "metadata": {},
   "source": [
    "2. Using the code chunks below, which computes the following:\n",
    "\n",
    "- identifies numeric, categorical, and ordinal columns in our full feature set,\n",
    "- replaces unique values in our ordinal columns (i.e. \"No_basement\", \"No_garage\"), and\n",
    "- creates our encoders for the numeric, categorical, and ordinal columns.\n",
    "\n",
    "<div class=\"admonition note alert alert-info\">\n",
    "    <p class=\"first admonition-title\" style=\"font-weight: bold;\"><b>Note</b></p>\n",
    "<p class=\"last\">Run the following two code cells without changing anything.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0cf7c8",
   "metadata": {
    "tags": [
     "ci-skip"
    ]
   },
   "outputs": [],
   "source": [
    "######## RUN THIS CODE CELL AS-IS ########\n",
    "\n",
    "# get columns of interest\n",
    "numerical_columns = num_features.columns\n",
    "ordinal_columns = cat_features.filter(regex='Qual').columns\n",
    "categorical_columns = cat_features.drop(columns=ordinal_columns).columns\n",
    "\n",
    "# replace unique values in our ordinal columns (i.e. \"No_basement\", \"No_garage\") with 'NA'\n",
    "for col in ordinal_columns:\n",
    "    features[col] = features[col].replace(to_replace='No_.*', value='NA', regex=True)\n",
    "    \n",
    "# split full feature set (numeric, categorical, & ordinal features) into train & test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713f1d19",
   "metadata": {
    "tags": [
     "ci-skip"
    ]
   },
   "outputs": [],
   "source": [
    "######## RUN THIS CODE CELL AS-IS ########\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# create our numeric, categorical, and ordinal preprocessor encoders\n",
    "numerical_preprocessor = StandardScaler()\n",
    "categorical_preprocessor = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "\n",
    "ordinal_categories = [\n",
    "    \"NA\", \"Very_Poor\", \"Poor\", \"Fair\", \"Below_Average\", \"Average\", \"Typical\",\n",
    "    \"Above_Average\", \"Good\", \"Very_Good\", \"Excellent\", \"Very_Excellent\"\n",
    "]\n",
    "list_of_ord_cats = [ordinal_categories for col in ordinal_columns]\n",
    "ordinal_preprocessor = OrdinalEncoder(categories=list_of_ord_cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0d4d19",
   "metadata": {},
   "source": [
    "2. Continued...\n",
    "\n",
    "Now fill in the blanks to create our `ColumnTransformer` that:\n",
    "\n",
    "- standardizes numerical columns (preprocessor: `numerical_preprocessor`; columns of interest: `numerical_columns`) \n",
    "- one-hot encodes categorical columns (preprocessor: `categorical_preprocessor`; columns of interest: `categorical_columns`) \n",
    "- ordinal encodes ordinal columns (preprocessor: `ordinal_preprocessor`; columns of interest: `ordinal_columns`) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3079666b",
   "metadata": {
    "tags": [
     "ci-skip"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('standard_scaler', __________, __________),\n",
    "    ('one_hot_encoder', __________, __________),\n",
    "    ('ordinal_encoder', __________, __________),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c8487e",
   "metadata": {},
   "source": [
    "3. Now create a pipeline that includes the preprocessing step and applies a linear regression model. Does this improve the linear regression's $R^2$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25c4be5",
   "metadata": {
    "tags": [
     "ci-skip"
    ]
   },
   "outputs": [],
   "source": [
    "lm_full = make_pipeline(___________, ___________)\n",
    "_ = lm_full.fit(X_train, y_train)\n",
    "lm_full.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8174aabe",
   "metadata": {},
   "source": [
    "4. If time allows, create a pipeline that applies these preprocessing steps with a default random forest model and see if performance improves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101c4572-0ce6-4436-9621-835ee6b5c872",
   "metadata": {},
   "source": [
    "### GitHub Check-in\n",
    "\n",
    "Add, commit (with a good message!), and push your code to this point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622f7037",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377cd13e",
   "metadata": {},
   "source": [
    "### Model evaluation & selection\n",
    "\n",
    "1. Using same preprocessing pipeline you created in Part 1, fit a default random forest model using a 5-fold cross validation procedure using the root mean squared error metric (`'neg_root_mean_squared_error'`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f5f15f",
   "metadata": {},
   "source": [
    "2. Run the following two code chunks as is without making any changes. This will create a random forest model pipeline and create specified hyperparameter distributions to draw from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2294d7b1",
   "metadata": {
    "tags": [
     "ci-skip"
    ]
   },
   "outputs": [],
   "source": [
    "######## RUN THIS CODE CELL AS-IS ########\n",
    "\n",
    "from scipy.stats import loguniform\n",
    "\n",
    "\n",
    "class loguniform_int:\n",
    "    \"\"\"Integer valued version of the log-uniform distribution\"\"\"\n",
    "    def __init__(self, a, b):\n",
    "        self._distribution = loguniform(a, b)\n",
    "\n",
    "    def rvs(self, *args, **kwargs):\n",
    "        \"\"\"Random variable sample\"\"\"\n",
    "        return self._distribution.rvs(*args, **kwargs).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2903d1",
   "metadata": {
    "tags": [
     "ci-skip"
    ]
   },
   "outputs": [],
   "source": [
    "######## RUN THIS CODE CELL AS-IS ########\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# create preprocessor & modeling pipeline\n",
    "rf = RandomForestRegressor(random_state=123)\n",
    "pipeline = Pipeline([('prep', preprocessor), ('rf', rf)])\n",
    "\n",
    "# specify hyperparameter distributions to randomly sample from\n",
    "param_distributions = {\n",
    "    'rf__n_estimators': loguniform_int(50, 1000),\n",
    "    'rf__max_features': loguniform(.1, .8),\n",
    "    'rf__max_depth': loguniform_int(2, 30),\n",
    "    'rf__min_samples_leaf': loguniform_int(1, 100),\n",
    "    'rf__max_samples': loguniform(.5, 1),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6771c59",
   "metadata": {},
   "source": [
    "2. Continued...\n",
    "\n",
    "Fill in the blanks to perform a random hyperparameter search based on the following:\n",
    "\n",
    "- use the parameter distributions specified above,\n",
    "- perform 25 random searches,\n",
    "- use a 5-fold cross-validation procedure, and\n",
    "- use root mean squared error (RMSE) as our scoring metric.\n",
    "\n",
    "What are the hyperparameters that provide the lowest RMSE? What is the lowest cross validated RMSE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40410327",
   "metadata": {
    "tags": [
     "ci-skip"
    ]
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import ___________\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    pipeline, \n",
    "    param_distributions=___________, \n",
    "    n_iter=__,\n",
    "    cv=__, \n",
    "    scoring='___________',\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "results = random_search.___________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa69e649",
   "metadata": {},
   "source": [
    "### Unit Tests\n",
    "\n",
    "1. TBD\n",
    "1. TBD\n",
    "1. TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98334504",
   "metadata": {},
   "source": [
    "### ML lifecycle management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c8a06f",
   "metadata": {},
   "source": [
    "1. Create and set an MLflow experiment titled \"UC Advanced Python Case Study\"\n",
    "2. Re-perform the random hyperparameter search executed above while logging the hyperparameter search experiment with MLflow's autologging. Title this run \"rf_hyperparameter_tuning\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60677940",
   "metadata": {},
   "source": [
    "### Reproducibility with dependency tracking\n",
    "\n",
    "1. TBD\n",
    "1. TBD\n",
    "1. TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8271687",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
